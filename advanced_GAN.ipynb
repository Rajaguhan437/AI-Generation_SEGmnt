{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision, os, PIL, pdb\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(tensor, num=25, wandbactive=0, name=''):\n",
    "    data = tensor.detach().cpu()\n",
    "    grid = make_grid(data[:num], nrow=5).permute(1,2,0)\n",
    "    \n",
    "    if  wandbactive==1 :\n",
    "        wandb.log({name:wandb.Image(grid.numpy().clip(0,1))})\n",
    "        \n",
    "    plt.imshow(grid.clip(0,1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 500\n",
    "batch_size = 128\n",
    "lr = 1e-4\n",
    "z_dim = 256\n",
    "device = \"cuda\"\n",
    "\n",
    "cur_step = 0\n",
    "crit_cycles = 5\n",
    "gen_losses = []\n",
    "critic_losses = []\n",
    "show_steps = 50\n",
    "save_steps = 100\n",
    "\n",
    "wandbact = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msparrowblack437\u001b[0m (\u001b[33mpeakysparrow\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Guhan/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key=\"95eda70b703e651a93829d62187e30b7bc107306\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "exp_name = wandb.util.generate_id()\n",
    "\n",
    "myrun = wandb.init(\n",
    "    project = \"wGAN\",\n",
    "    group = \"exp_name\",\n",
    "    config = {\n",
    "        \"optimizer\" : \"sgd\",\n",
    "        \"model\" : \"wgan_gp\",\n",
    "        \"epoch\" : \"1000\",\n",
    "        \"batch_size\" : 128\n",
    "        \n",
    "    }\n",
    ")\n",
    "\n",
    "config = wandb.config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4c1z4v63\n"
     ]
    }
   ],
   "source": [
    "print(exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim=64, d_dim=16):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.z_dim = z_dim\n",
    "        \n",
    "        self.gen = nn.Sequential(\n",
    "            ###img_shape = n-1 * stride - 2*padding + kernel_size\n",
    "            ###init_img_shape = 1 x 1\n",
    "            nn.ConvTranspose2d(z_dim, d_dim*32, 4, 1, 0), # = 4x4 image with ch 256 => 512\n",
    "            nn.BatchNorm2d(d_dim * 32),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(d_dim*32, d_dim*16, 4, 2, 1), # = 8x8 image with ch 512 => 256\n",
    "            nn.BatchNorm2d(d_dim * 16),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(d_dim*16, d_dim*8, 4, 2, 1), # = 16x16 image with ch 256 => 128\n",
    "            nn.BatchNorm2d(d_dim * 8),\n",
    "            nn.ReLU(True),            \n",
    "                        \n",
    "            nn.ConvTranspose2d(d_dim*8, d_dim*4, 4, 2, 1), # = 32x32 image with ch 128 => 64\n",
    "            nn.BatchNorm2d(d_dim * 4),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(d_dim*4, d_dim*2, 4, 2, 1), # = 64x64 image with ch 64 => 32\n",
    "            nn.BatchNorm2d(d_dim * 2),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(d_dim*2, 3, 4, 2, 1), # = 128x128 image with ch 32 => 3\n",
    "            nn.Tanh() #  produce result in the range -1 to 1     \n",
    "                             \n",
    "        ) \n",
    "        \n",
    "        def forward(self, noise):\n",
    "            x = noise.view(len(noise), self.z_dim, 1, 1)\n",
    "            return self.gen(x)\n",
    "\n",
    "def gen_noise(num, z_dim, device=\"cuda\"):\n",
    "    return torch.randn(num, z_dim, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, d_dim=16):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            # img_shape = (n+2*pad-ker_size) // stride+1\n",
    "            # init_img_shape = 128x128\n",
    "            nn.Conv2d(3, d_dim, 4, 2, 1),  # = 64x64 image with ch 3 => 16\n",
    "            nn.InstanceNorm2d(d_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv2d(d_dim, d_dim*2, 4, 2, 1),  # = 32x32 image with ch 16 => 32\n",
    "            nn.InstanceNorm2d(d_dim*2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv2d(d_dim*2, d_dim*4, 4, 2, 1),  # = 16x16 image with ch 32 => 64\n",
    "            nn.InstanceNorm2d(d_dim*4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv2d(d_dim*4, d_dim*8, 4, 2, 1),  # = 8x8 image with ch 64 => 128\n",
    "            nn.InstanceNorm2d(d_dim*8),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv2d(d_dim*8, d_dim*16, 4, 2, 1),  # = 4x4 image with ch 128 => 256\n",
    "            nn.InstanceNorm2d(d_dim*16),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv2d(d_dim*16, 1, 4, 1, 0),  # = 1x1 image with ch 256 => 1\n",
    "            \n",
    "        )\n",
    "        \n",
    "    def forward(self, image):\n",
    "        # image : 128 x 3 x 128 x 128\n",
    "        critic_pred = self.critic(image) # == 128 x 1 x 1 x 1 \n",
    "        return critic_pred.view(len(critic_pred), -1) # 128 x 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## intit weights for layers \n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0) \n",
    "        \n",
    "#gen = gen.apply(init_weights)\n",
    "#critic = critic.apply(inint_weights)    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "import gdown, zipfile\n",
    "\n",
    "url = \"https://drive.google.com/drive/folders/0B7EVK8r0v71pTUZsaXdaSnZBZzg?resourcekey=0-rJlzl934LzC-Xp28GeIBzQ\"\n",
    "path = 'G:/Datasets/celeba'\n",
    "download_path = path+'/img_align_celeba.zip'\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "gdown.download(url, download_path, quiet = False)\n",
    "\n",
    "with zipfile.ZipFile(download_path, 'r') as Ziphandler:\n",
    "    Ziphandler.extractall(path)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, path, size=128, lim=1000):\n",
    "        self.sizes = [size, size]\n",
    "        items, labels = [], []\n",
    "        \n",
    "        for data in os.listdir(path)[:lim]:\n",
    "            item = os.path.join(path, data)\n",
    "            items.append(item)\n",
    "            labels.append(data)\n",
    "        self.items = items\n",
    "        self.labes = labels\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.items)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            data = PIL.Image.open(self.items[idx]).convert('RGB') # (width, height)\n",
    "            data = np.asarray(torchvision.transforms.Resize(self.sizes)(data)) # 128 x 128 x 3\n",
    "            data = np.transpose(data, (2, 0, 1)).astype(np.float32, copy=False) # 3 x 128 x 128\n",
    "            data = torch.from_numpy(data).div(255) # from 0 to 1\n",
    "            return data, self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data\n",
    "\n",
    "data_path = download_path = path+'/img_align_celeba'\n",
    "ds = Dataset(data_path, size=128, lim=10000)\n",
    "\n",
    "## DataLoader\n",
    "\n",
    "dataloader = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "## Models\n",
    "\n",
    "gen = Generator(z_dim).to(device)\n",
    "critic = Critic().to(device)\n",
    "\n",
    "## Optimizer\n",
    "\n",
    "gen_opt = torch.optim.Adam(gen.parameters(), lr = lr, betas=(0.5, 0.9))\n",
    "critic_opt = torch.optim.Adam(critic.parameters(), lr = lr, betas=(0.5, 0.9))\n",
    "\n",
    "## Initialisations\n",
    "\n",
    "#gen = gen.apply(init_weights)\n",
    "#critic = critic.apply(inint_weights) \n",
    "\n",
    "# Wandb\n",
    "\n",
    "if wandbact == 1:\n",
    "    wandb.watch(gen, log_freq=100)\n",
    "    wandb.watch(critic, log_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test\n",
    "\n",
    "x, y = next(iter(dataloader))\n",
    "show(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## gradient penalty calaculation\n",
    "\n",
    "def get_gp(real, fake, critic, alpha, gamma=10):\n",
    "    mix_images = real * alpha + fake * (1 - alpha) # 128 x 1 x 128 x 128 , linear interpolation\n",
    "    mix_scores = critic(mix_images) # 128 x 1\n",
    "    \n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs = mix_images,\n",
    "        outputs = mix_scores,\n",
    "        grad_outputs = torch.ones_like(mix_scores),\n",
    "        retain_graph = True,\n",
    "        create_graph = True\n",
    "    )[0]  # 128 x 3 x 128 x 128\n",
    "    \n",
    "    gradient = gradient.view(len(gradient), -1) # 128 x 3*128*128\n",
    "    gradient_norm = gradient.norm(2, dim = 1)\n",
    "    gp = gamma * ((gradient_norm - 1)**2).mean()\n",
    "    \n",
    "    return gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save checkpoints \n",
    "\n",
    "save_path = \"G:/Software/\"\n",
    "\n",
    "def save_checkpoint(name):\n",
    "    \n",
    "    ## Gen model\n",
    "    torch.save({\n",
    "        'epoch' : epoch, \n",
    "        'model_state_dict' :gen.state_dict(),\n",
    "        'optimizer_state_dict' : gen_opt.state_dict()\n",
    "    }, save_path+'G-'+name+'.pkl')\n",
    "    \n",
    "    \n",
    "    ## Critic Model\n",
    "    torch.save({\n",
    "        'epoch' : epoch, \n",
    "        'model_state_dict' :critic.state_dict(),\n",
    "        'optimizer_state_dict' : critic_opt.state_dict()\n",
    "    }, save_path+'C-'+name+'.pkl')\n",
    "    \n",
    "    print(\"Saved Checkpoint\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Checkpoint\n",
    "\n",
    "def load_checkpoint(name):\n",
    "    \n",
    "    ## Load Gen\n",
    "    chkpt = torch.load(save_path+'G-'+name+'.pkl')\n",
    "    gen.load_state_dict(chkpt['model_state_dict'])\n",
    "    gen_opt.load_state_dict(chkpt['optimizer_state_dict'])\n",
    "    \n",
    "    ## Load Critic\n",
    "    chkpt = torch.load(save_path+'C-'+name+'.pkl')\n",
    "    critic.load_state_dict(chkpt['model_state_dict'])\n",
    "    critic_opt.load_state_dict(chkpt['optimizer_state_dict'])\n",
    "    \n",
    "    print('Loaded Checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test save and load\n",
    "\n",
    "epoch = 1\n",
    "save_checkpoint(\"test\")\n",
    "\n",
    "load_checkpoint(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    for real, _ in tqdm(dataloader):\n",
    "        cur_bs = len(real) # 128\n",
    "        real = real.to(device)\n",
    "        \n",
    "        ## Critic\n",
    "        mean_critic_loss = 0\n",
    "        \n",
    "        for _ in range(crit_cycles):\n",
    "            critic_opt.zero_grad()\n",
    "            \n",
    "            noise = gen_noise(cur_bs, z_dim)\n",
    "            fake = gen(noise)\n",
    "            critic_fake_pred = critic(fake.detach())\n",
    "            critic_real_pred = critic(real)\n",
    "            \n",
    "            alpha = torch.rand(len(real), 1, 1, 1, device=device, requires_grad=True) # 128 x 1 x 1 x 1\n",
    "            gp = get_gp(real, fake.detach(), critic, alpha)\n",
    "            \n",
    "            critic_loss = critic_fake_pred.mean() - critic_real_pred.mean() + gp \n",
    "            \n",
    "            mean_critic_loss += critic_loss.item() / crit_cycles\n",
    "            \n",
    "            critic_loss.backward(retain_graph = True)\n",
    "            critic_opt.step()\n",
    "            \n",
    "        critic_losses += [mean_critic_loss]\n",
    "        \n",
    "        ## Generator\n",
    "        \n",
    "        gen_opt.zero_grad()\n",
    "        noise = gen_noise(cur_bs, z_dim)\n",
    "        fake = gen(noise)\n",
    "        critic_fake_pred = critic(fake)\n",
    "        \n",
    "        gen_loss = critic_fake_pred.mean()\n",
    "        gen_loss.backward()\n",
    "        gen_opt.step()\n",
    "        \n",
    "        gen_losses += [gen_loss.item()]\n",
    "        \n",
    "        ### Stats\n",
    "        \n",
    "        if wandbact == 1:\n",
    "            wandb.log('Epoch =', epoch, 'Step =', cur_step, \"Critic_Loss =\", mean_critic_loss, \"Gen_Loss =\", gen_loss )\n",
    "            \n",
    "            \n",
    "        if cur_step % save_steps == 0 and cur_step > 0:\n",
    "            print(\"Saving Checkpoint:\", cur_step, save_steps)\n",
    "            save_checkpoint(\"wGAN-CelebFace\")\n",
    "            \n",
    "        if cur_step % show_steps == 0 and cur_step > 0:\n",
    "            show(fake, wandbactive=1 , name='fake')\n",
    "            show(real, wandbactive=1 , name='real')\n",
    "            \n",
    "            gen_mean = sum(gen_losses[-show_steps:]) / show_steps\n",
    "            critic_mean = sum(critic_losses[-show_steps:]) / show_steps\n",
    "            print('Epoch =', epoch, 'Step =', cur_step, \"Critic_Loss =\", critic_mean, \"Gen_Loss =\", gen_mean )\n",
    "            \n",
    "            plt.plot(\n",
    "                range(len(gen_losses)),\n",
    "                torch.Tensor(gen_losses),\n",
    "                label=\"Generator Loss\"\n",
    "            )\n",
    "            \n",
    "            plt.plot(\n",
    "                range(len(gen_losses)),\n",
    "                torch.Tensor(critic_losses),\n",
    "                label=\"Critic Loss\"\n",
    "            )\n",
    "            \n",
    "            plt.ylim(-1000, 1000)\n",
    "            plt.legend()\n",
    "            plt.show()   \n",
    "            \n",
    "        cur_step += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate faces\n",
    "\n",
    "noise = gen_noise(batch_size, z_dim)\n",
    "fake = gen(noise)\n",
    "show(fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(fake[4].detach().cpu().permute(1, 2, 0).squeeze().clip(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# morphing with interpolation between random points in latent space\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "gen_set = []\n",
    "z_shape = [1 , z_dim, 1, 1]\n",
    "rows = 4\n",
    "steps = 20\n",
    "\n",
    "for i in range(rows):\n",
    "    z1, z2 = torch.randn(z_shape), torch.randn(z_shape)\n",
    "    for alpha in np.linspace(0, 1, steps):\n",
    "        z = alpha*(z1) + (1 - alpha) * z2\n",
    "        res = gen(z.cuda())[0]\n",
    "        gen_set.append(res)\n",
    "\n",
    "fig = plt.figure(figsize=(25, 11))\n",
    "grid = ImageGrid(fig, 111, nrows_ncols = (rows, steps), axes_pad = 0.1)\n",
    "\n",
    "for ax, img in grid(gen_set):\n",
    "    ax.axis('off')\n",
    "    res = img.cpu().detach().permute(1, 2, 0)\n",
    "    res =  res - res.min()\n",
    "    res = res/res.max() - res.min() \n",
    "    ax.imshow(res)\n",
    "plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
